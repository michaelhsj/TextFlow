{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b394c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import attrs\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "NUM_IMAGES = 100\n",
    "IMG_ROOT = Path(\"../../dataset/TextOCR/train_val_images\")\n",
    "TOCR_JSON = Path(\"../../dataset/TextOCR/TextOCR_0.1_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/yunusserhat/TextOCR-Dataset\n",
    "tocr_json = json.load(open(TOCR_JSON))\n",
    "# {\n",
    "#   \"imgs\": {\n",
    "#     \"OpenImages_ImageID_1\": {\n",
    "#       \"id\": \"OpenImages_ImageID_1\",\n",
    "#       \"width\":  W,\n",
    "#       \"height\": H,\n",
    "#       \"set\": \"train|val|test\",\n",
    "#       \"filename\": \"train|test/OpenImages_ImageID_1.jpg\"\n",
    "#     }\n",
    "#   },\n",
    "#   \"anns\": {\n",
    "#     \"OpenImages_ImageID_1_1\": {\n",
    "#       \"id\": \"OpenImages_ImageID_1_1\",\n",
    "#       \"image_id\": \"OpenImages_ImageID_1\",\n",
    "#       \"bbox\": [x1, y1, x2, y2],\n",
    "#       \"points\": [x1, y1, x2, y2, ..., xN, yN],\n",
    "#       \"utf8_string\": \"text\",\n",
    "#       \"area\": A\n",
    "#     }\n",
    "#   },\n",
    "#   \"img2Anns\": {\n",
    "#     \"OpenImages_ImageID_1\": [\"OpenImages_ImageID_1_1\", \"...\"]\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ad80cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torchvision.io import decode_image\n",
    "from typing import Any\n",
    "\n",
    "class TextOCRDoctrDetDataset(Dataset[tuple[Tensor, Tensor]]):\n",
    "\n",
    "    def __init__(self, images_dir: Path, json_path: Path, num_samples: int):\n",
    "        json_data = tocr_json or json.load(open(json_path))\n",
    "\n",
    "        # Maximum number of labels and points for padding\n",
    "        max_labels, max_pts = self.get_max_dimensions(json_data) \n",
    "\n",
    "        self.image_paths: list[Path] = []\n",
    "        self.annotations: Tensor = torch.zeros((num_samples, max_labels, max_pts, 2))\n",
    "\n",
    "        sample_num = 0\n",
    "        for img_id, ann_ids in json_data[\"imgToAnns\"].items():\n",
    "            if sample_num >= num_samples:\n",
    "                break\n",
    "\n",
    "            # Construct image path\n",
    "            tocr_img_json = json_data[\"imgs\"][img_id]\n",
    "            self.image_paths.append(images_dir/tocr_img_json[\"file_name\"])\n",
    "\n",
    "            # Convert image annotations to Tensors\n",
    "            img_id=tocr_img_json[\"id\"]\n",
    "\n",
    "            for i, ann_id in enumerate(ann_ids):\n",
    "                tocr_ann_json = json_data[\"anns\"][ann_id]\n",
    "                points: list[float] = tocr_ann_json['points']\n",
    "                self.annotations[sample_num][i] = self.points_to_tensor(points, padding = max_pts)\n",
    "            sample_num += 1\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Tensor, Tensor]:\n",
    "        image = decode_image(str(self.image_paths[idx]))\n",
    "        return image, self.annotations[idx]\n",
    "    \n",
    "    def points_to_tensor(self, point_values: list[float], padding: int)->Tensor:\n",
    "        points: list[tuple[float, float]] = [(0,0)]*padding\n",
    "        for i in range(len(point_values)//2):\n",
    "            points[i] = (point_values[2*i], point_values[2*i+1])\n",
    "        return torch.tensor(points)\n",
    "    \n",
    "    def get_max_dimensions(self, json_data: Any) -> tuple[int,int]:\n",
    "        max_labels = 0\n",
    "        for ann_ids in json_data[\"imgToAnns\"].values():\n",
    "            max_labels = max(max_labels, len(ann_ids))\n",
    "\n",
    "        max_pts = 0\n",
    "        for ann in json_data[\"anns\"].values():\n",
    "            max_pts = max(max_pts, len(ann['points']))\n",
    "\n",
    "        return max_labels, max_pts\n",
    "\n",
    "dataset = TextOCRDoctrDetDataset(IMG_ROOT, TOCR_JSON, NUM_IMAGES)\n",
    "loader = DataLoader(dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "63e75640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "from torchvision.transforms.functional import to_pil_image, pil_to_tensor\n",
    "\n",
    "def clean_polygons(polys_p_n_2: torch.Tensor) -> list:\n",
    "    \"\"\"\n",
    "    Remove zero-padded points from polygon tensors.\n",
    "    polys_p_n_2: [P, N, 2]\n",
    "    Returns: list of (N_i, 2) numpy arrays (valid polygons)\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for poly in polys_p_n_2:\n",
    "        # keep only points that are not [0,0]\n",
    "        mask = ~torch.all(poly == 0, dim=-1)\n",
    "        valid = poly[mask]\n",
    "        if len(valid) >= 3:  # at least a triangle\n",
    "            cleaned.append(valid.numpy())\n",
    "    return cleaned\n",
    "\n",
    "def overlay_polygons(img_chw_uint8: torch.Tensor,\n",
    "                     polys_p_n_2: torch.Tensor,\n",
    "                     alpha: float = 0.5) -> torch.Tensor:\n",
    "\n",
    "    _, H, W = img_chw_uint8.shape\n",
    "    polys = clean_polygons(polys_p_n_2)\n",
    "\n",
    "    masks = torch.zeros((len(polys), H, W), dtype=torch.bool)\n",
    "    for i, poly in enumerate(polys):\n",
    "        m = Image.new(mode=\"1\", size=(W, H), color=0)\n",
    "        d = ImageDraw.Draw(m)\n",
    "        d.polygon([tuple(p) for p in poly], fill=1)\n",
    "        masks[i] = torch.from_numpy(np.array(m, dtype=bool))\n",
    "\n",
    "    colors = [\"red\", \"lime\", \"blue\", \"yellow\", \"magenta\", \"cyan\"]\n",
    "    return draw_segmentation_masks(img_chw_uint8, masks,\n",
    "                                   colors=[colors[i % len(colors)] for i in range(len(polys))],\n",
    "                                   alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "image, polygons = next(iter(loader))\n",
    "overlay = overlay_polygons(image[0], polygons[0], alpha=0.4)\n",
    "\n",
    "to_pil_image(overlay).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-textflow (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
