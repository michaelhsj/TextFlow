{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b394c44d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "from torchvision.utils import draw_segmentation_masks\n",
        "from IPython.display import display\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import Tensor\n",
        "import torch\n",
        "from torchvision.io import decode_image\n",
        "from torchvision.transforms.functional import resize\n",
        "import attrs\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "NUM_IMAGES = 100\n",
        "TARGET_IMAGE_SIZE = [1024, 1024]\n",
        "DATASET_DIR = Path(\"../../dataset/TextOCR\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950e4b83",
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://huggingface.co/datasets/yunusserhat/TextOCR-Dataset\n",
        "tocr_json = json.load(open(DATASET_DIR / \"TextOCR_0.1_train.json\"))\n",
        "# {\n",
        "#   \"imgs\": {\n",
        "#     \"OpenImages_ImageID_1\": {\n",
        "#       \"id\": \"OpenImages_ImageID_1\",\n",
        "#       \"width\":  W,\n",
        "#       \"height\": H,\n",
        "#       \"set\": \"train|val|test\",\n",
        "#       \"filename\": \"train|test/OpenImages_ImageID_1.jpg\"\n",
        "#     }\n",
        "#   },\n",
        "#   \"anns\": {\n",
        "#     \"OpenImages_ImageID_1_1\": {\n",
        "#       \"id\": \"OpenImages_ImageID_1_1\",\n",
        "#       \"image_id\": \"OpenImages_ImageID_1\",\n",
        "#       \"bbox\": [x1, y1, x2, y2],\n",
        "#       \"points\": [x1, y1, x2, y2, ..., xN, yN],\n",
        "#       \"utf8_string\": \"text\",\n",
        "#       \"area\": A\n",
        "#     }\n",
        "#   },\n",
        "#   \"img2Anns\": {\n",
        "#     \"OpenImages_ImageID_1\": [\"OpenImages_ImageID_1_1\", \"...\"]\n",
        "#   }\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad80cd96",
      "metadata": {},
      "outputs": [],
      "source": [
        "@attrs.frozen\n",
        "class DoctrDetSample:\n",
        "    image_path: Path\n",
        "    width: int\n",
        "    height: int\n",
        "    polygons: np.ndarray\n",
        "\n",
        "\n",
        "class TextOCRDoctrDetDataset(Dataset[tuple[Tensor, dict[str, np.ndarray]]]):\n",
        "    \"\"\"Minimal TextOCR dataset wrapper emitting Doctr detection targets.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        images_dir: Path | None = None,\n",
        "        json_path: Path | None = None,\n",
        "        num_samples: int | None = None,\n",
        "    ) -> None:\n",
        "        base_images_dir = images_dir or (DATASET_DIR / \"train_val_images\")\n",
        "        metadata_path = json_path or (DATASET_DIR / \"TextOCR_0.1_train.json\")\n",
        "\n",
        "        with open(metadata_path) as fh:\n",
        "            json_data = tocr_json or json.load(fh)\n",
        "\n",
        "        self.samples: list[DoctrDetSample] = []\n",
        "        max_labels, max_pts = self.get_max_dimensions(json_data)\n",
        "\n",
        "        for img_id, ann_ids in json_data[\"imgToAnns\"].items():\n",
        "            if num_samples is not None and len(self.samples) >= num_samples:\n",
        "                break\n",
        "\n",
        "            img_meta = json_data[\"imgs\"][img_id]\n",
        "            width, height = img_meta[\"width\"], img_meta[\"height\"]\n",
        "\n",
        "            polygons: np.ndarray = np.zeros((max_labels, max_pts, 2), dtype=np.float32)\n",
        "            for i, ann_id in enumerate(ann_ids):\n",
        "                ann = json_data[\"anns\"][ann_id]\n",
        "                raw_points: list[float] = ann[\"points\"]\n",
        "\n",
        "                if not raw_points:\n",
        "                    continue\n",
        "                    \n",
        "                xs = np.array(raw_points[0::2], dtype=np.float32)\n",
        "                ys = np.array(raw_points[1::2], dtype=np.float32)\n",
        "\n",
        "                # Normalize\n",
        "                xs /= width\n",
        "                ys /= height\n",
        "\n",
        "                # Clip to image\n",
        "                xs = np.clip(xs, 0.0, float(TARGET_IMAGE_SIZE[0]))\n",
        "                ys = np.clip(ys, 0.0, float(TARGET_IMAGE_SIZE[1]))\n",
        "\n",
        "                # Stack and pad to the max number of polygon points in dataset\n",
        "                stacked = np.stack([xs,ys], axis=1)\n",
        "                polygons[i] = np.pad(stacked, pad_width=((0, max_pts-len(xs)), (0, 0)))\n",
        "\n",
        "            sample = DoctrDetSample(\n",
        "                image_path=Path(base_images_dir) / img_meta[\"file_name\"],\n",
        "                width=width,\n",
        "                height=height,\n",
        "                polygons=polygons,\n",
        "            )\n",
        "            self.samples.append(sample)\n",
        "\n",
        "        if not self.samples:\n",
        "            raise ValueError(\n",
        "                \"No samples found. Ensure the TextOCR dataset is available at\"\n",
        "                f\" {base_images_dir} and json at {metadata_path}\"\n",
        "            )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple[Tensor, dict[str, np.ndarray]]:\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        image = decode_image(str(sample.image_path))\n",
        "        image = resize(image, list(TARGET_IMAGE_SIZE), antialias=True)\n",
        "        image = image.to(torch.float32) / 255.0\n",
        "\n",
        "        # Doctr expects a dict keyed by class name containing relative boxes in float32 numpy.\n",
        "        target = {\"words\": sample.polygons.copy()}\n",
        "        return image, target\n",
        "    \n",
        "    def get_max_dimensions(self, json_data: Any) -> tuple[int,int]:\n",
        "        max_labels = 0\n",
        "        for ann_ids in json_data[\"imgToAnns\"].values():\n",
        "            max_labels = max(max_labels, len(ann_ids))\n",
        "\n",
        "        max_pts = 0\n",
        "        for ann in json_data[\"anns\"].values():\n",
        "            max_pts = max(max_pts, len(ann['points']))\n",
        "\n",
        "        return max_labels, max_pts\n",
        "\n",
        "\n",
        "dataset = TextOCRDoctrDetDataset(num_samples=NUM_IMAGES)\n",
        "loader = DataLoader(dataset, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e75640",
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_polygons(polys_p_n_2: torch.Tensor) -> list[Tensor]:\n",
        "    \"\"\"\n",
        "    Remove zero-padded points from polygon tensors.\n",
        "    polys_p_n_2: [P, N, 2]\n",
        "    Returns: list of (N_i, 2) numpy arrays (valid polygons)\n",
        "    \"\"\"\n",
        "    cleaned: list[Tensor] = []\n",
        "    for poly in polys_p_n_2:\n",
        "        # keep only points that are not [0,0]\n",
        "        mask = ~torch.all(poly == 0, dim=-1)\n",
        "        valid = poly[mask]\n",
        "        if len(valid) >= 3:  # at least a triangle\n",
        "            cleaned.append(valid)\n",
        "    return cleaned\n",
        "\n",
        "def overlay_polygons(image: torch.Tensor,\n",
        "                     polygons: torch.Tensor,\n",
        "                     alpha: float = 0.5) -> torch.Tensor:\n",
        "\n",
        "    _, H, W = image.shape\n",
        "    unpadded_polys = clean_polygons(polygons)\n",
        "    if not unpadded_polys:\n",
        "        return image\n",
        "\n",
        "    unpadded_polys = [p*TARGET_IMAGE_SIZE[0] for p in unpadded_polys]\n",
        "    \n",
        "    masks = torch.zeros((len(unpadded_polys), H, W), dtype=torch.bool)\n",
        "    for i, poly in enumerate(unpadded_polys):\n",
        "        m = Image.new(mode=\"1\", size=(W, H), color=0)\n",
        "        d = ImageDraw.Draw(m)\n",
        "        d.polygon([p.tolist() for p in poly], fill=1)\n",
        "        masks[i] = torch.from_numpy(np.array(m, dtype=bool))\n",
        "\n",
        "    colors = [\"red\", \"lime\", \"blue\", \"yellow\", \"magenta\", \"cyan\"]\n",
        "    return draw_segmentation_masks(image, masks,\n",
        "                                   colors=[colors[i % len(colors)] for i in range(len(unpadded_polys))],\n",
        "                                   alpha=alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed0441c",
      "metadata": {},
      "outputs": [],
      "source": [
        "images, targets = next(iter(loader))\n",
        "sample_image = images[0]\n",
        "sample_polygons = targets['words'][0]\n",
        "overlay = overlay_polygons(sample_image, sample_polygons, alpha=0.4)\n",
        "display(to_pil_image(overlay))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-textflow (3.11.8)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
